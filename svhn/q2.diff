diff -u ori/train.py q2/train.py
--- ori/train.py	2019-06-05 03:11:02.000000000 +0800
+++ q2/train.py	2020-04-08 11:45:11.000000000 +0800
@@ -33,10 +33,9 @@
 def main():
     parser = argparse.ArgumentParser()
     parser.add_argument('-c', '--continue', dest='continue_path', required=False)
-    parser.add_argument('-l', '--loss', default='softmax')
+    parser.add_argument('-l', '--loss', default='regression')
     args = parser.parse_args()
 
-    assert args.loss in ['softmax', 'abs-max', 'square-max', 'plus-one-abs-max', 'non-negative-max']
 
     ## load dataset
     train_batch_gnr, train_set = get_dataset_batch(ds_name='train')
@@ -46,7 +45,7 @@
     network = Model()
     placeholders, label_onehot, logits = network.build()
 
-    if args.loss == 'softmax':
+    if args.loss == 'softmax' or args.loss == 'regression':
         preds = tf.nn.softmax(logits)
     elif args.loss == 'abs-max':
         abs_logits = tf.abs(logits)
@@ -67,7 +66,10 @@
                             tf.cast(tf.argmax(label_onehot, 1), dtype=tf.int32))
     accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
     loss_reg = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
-    loss = tf.losses.softmax_cross_entropy(label_onehot, logits) + loss_reg
+    if args.loss == 'softmax':
+        loss = tf.losses.softmax_cross_entropy(label_onehot, logits) + loss_reg
+    else:
+        loss = tf.reduce_mean(tf.reduce_sum(tf.square(preds - tf.cast(label_onehot, dtype=tf.float32)), axis=1))
 
     ## train config
     global_steps = tf.Variable(0, trainable=False)
